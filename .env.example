# cp .env.example .env
# Edit your .env file with your own values
# Don't commit your .env file to git/push to GitHub!
# Don't modify/delete .env.example unless adding extensions to the project
# which require new variable to be added to the .env file

# ----------------------------------
# API CONFIG
# OPENAI_API_MODEL can be used instead
# Special values:
# human - use human as intermediary with custom LLMs
# llama - use llama.cpp with Llama, Alpaca, Vicuna, GPT4All, etc.
# ----------------------------------
LLM_MODEL=gpt-3.5-turbo
MAX_TOKENS=2000                 # OpenAI: 2000 / 7B-Llama: 1000

# OPENAI API CONFIG
OPENAI_API_KEY=
OPENAI_API_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.5

# LLAMA model config (enhanced parameters, used for Llama only)
LLAMA_TEMPERATURE=0.8           # default: 0.2 (values between 0.7 to 0.9 seem to work best for 7B-Llama)
LLAMA_CTX_MAX=2048              # 7B-Llama: 2048 (4096 works as well, but w/o improvement. My understanding is that the context size for Llama is 2048.)
LLAMA_THREADS_NUM=8             # default: 8
LLAMA_MODEL_PATH=/Users/robiwan/models/ggml-wizardLM-7B.q4_2.bin

# The context limit is in chars, not tokens, which is roughly factor 4
# Further factors are applied on-top for e.g. document embedding context, task context, task list, task result, etc.
LLAMA_CONTEXT=8000              # 7B-Llama: 8000

# On-top factors for all types of context (smart context limitation)
# I have added the parameters for easy experimentation, but the default values should work well, at least with 7B-Llama
TASK_LIST_FACTOR=0.35           # default: 0.3 (task creation agent)
TASK_RESULT_FACTOR=0.45         # default: 0.5 (task creation agent)
TASK_DESCRIPTION_FACTOR=0.2     # default: 0.2 (task creation agent)
TASK_NAME_FACTOR=0.6            # default: 0.7 (task prioritization agent)
TASK_CONTEXT_FACTOR=0.25        # default: 0.25 (task execution agent)
SUMMARY_RESULT_FACTOR=0.7       # default: 1.0 (internet agent)
DOC_CONTEXT_FACTOR=0.4          # default: 0.4 (document embedding Q&A retrieval context, see extension ENABLE_DOCUMENT_EXTENSION)

# ----------------------------------
# STORE CONFIG
# TABLE_NAME can be used instead
# ----------------------------------
RESULTS_STORE_NAME=babyagi-db   # for chromadb directory 'chroma' is used instead

# Pinecone config
# Uncomment and fill these to switch from local ChromaDB to Pinecone
# PINECONE_API_KEY=
# PINECONE_ENVIRONMENT=

# Weaviate config
# Uncomment and fill these to switch from local ChromaDB to Weaviate
# WEAVIATE_USE_EMBEDDED=true
# WEAVIATE_URL=
# WEAVIATE_API_KEY=

# ----------------------------------
# COOPERATIVE MODE CONFIG
# BABY_NAME can be used instead
# ----------------------------------
INSTANCE_NAME=BabyAGI
COOPERATIVE_MODE=none

# RUN CONFIG
OBJECTIVE=Compare the development of past civilizations, their rise and fall and the factors involved, with the development of modern civilization and drwa conclusions.

# For backwards compatibility
# FIRST_TASK can be used instead of INITIAL_TASK
INITIAL_TASK=Develop a task list

# Extensions
# List additional extension .env files to load (except .env.example!)
DOTENV_EXTENSIONS=

# Set to true to enable command line args support
ENABLE_COMMAND_LINE_ARGS=false


# Write output to file (Experimental)
# Intended for step-by-step creation of a summary, story, report, etc. for the objective
# TODO: Add report parsing, processing, update and summarization functionality
# ----------------------------------
ENABLE_REPORT_EXTENSION=true   # default: false (experimental, but working nonetheless)
REPORT_FILE=report.txt

# Action to be performed for the OBJECTIVE, e.g. create a report, story or code
# The statement "code block" makes the LLM start/end the output with "‘‘‘" (triple backticks)
# Other option is to define a start tag as e.g. '###'
# ACTION=Write a python script and output in a code block.
ACTION=Create a report, start output for the report with '###'

# ----------------------------------
# Internet smart search extension (based on smart search from BabyCatAGI)
# SERPAPI, Google CSE or browser search possible (works also w/o any API key!)
# Fallback strategy for missing API key and API rate limit
# Summarization of web scraping results with OpenAI or Llama
# ----------------------------------
ENABLE_SEARCH_EXTENSION=true
INITIAL_SEARCH=false         # default: false (perform inital smart internet search & scrape and store results in document embedding store)
WIKI_SEARCH=false           # default: false (use wikipedia API instead of internet search, if better suited for the task, or use alone w/o internet search)
GOOGLE_API_KEY=
GOOGLE_CSE_ID=
SERPAPI_API_KEY=

# Temperature and CTX value for embedding LLM
SUMMARY_TEMPERATURE=0.7     # OpenAI: 0.7
SUMMARY_CTX_MAX=1024        # default: 1024

# Context size limit and webpage scrape length limits for smart internet search (both limits in chars, not tokens)
SUMMARY_CONTEXT=3000        # OpenAI: 3000 (value of 1000 works fine or 7B-Llama)
SCRAPE_LENGTH=5000          # OpenAI: 5000 (value of 3000 is a good trade-off for 7B-Llama)

# Search summary model path (default: leave empty for gpt-3.5-turbo)
SUMMARY_MODEL_PATH=

# ----------------------------------
# Document embedding extension using langchain, chromadb and local LLM, with additional features for persistent entity memory
# The document embedding functionality is from: https://github.com/imartinez/privateGPT.git
# Many thanks to https://github.com/imartinez for the great work!
# Relevant content from file privateGPT.py has integrated in BabyAGI
# The file ingest.py has been slightly adapted
# File scraper.py has been added for scraping a list of urls and store page content to files for use with ingest.py
# The sub-features EMBEDDINGS_BACKUP, EMBEDDINGS_UPDATE and WIKI_CONTEXT can be enabled independently from ENABLE_DOCUMENT_EXTENSION
# ----------------------------------
ENABLE_EMBEDDINGS_EXTENSION=false       # default: false (the document embeddings vectorstore has to be created before enabling the parameter!)
EMBEDDINGS_BACKUP=true                  # default: true (write enriched task results to file as "memory" backup)
EMBEDDINGS_UPDATE=true                  # default: true (create/Update document embeddings vectorstore with internet scrape data, used as additional context)
WIKI_CONTEXT=false                      # default: false (use wikipedia API as additional context)

# Document embeddings store config
EMBEDDINGS_STORE_NAME=chroma-memory     # name for document vectorstore
DOC_SOURCE_PATH=source_documents        # path to source documents
SCRAPE_SOURCE_PATH=scrape_documents     # path to scrape documents

# Document embedding model config
EMBEDDINGS_CTX_MAX=1024                 # default: 1024
EMBEDDINGS_TEMPERATURE=0.8              # default: 0.8 (LlamaCpp) or 0.7 (GPT4All)
EMBEDDINGS_MODEL_TYPE=LlamaCpp          # LlamaCpp or GPT4All
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2  # default: all-MiniLM-L6-v2
EMBEDDINGS_MODEL_PATH=/Add/path/to/model       # default: has to be set according to used Llama model

